{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e12e454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "q = pd.read_parquet('./data_mms/q_f2k_20unk.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f320a4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(415317, 41)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c151530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['request_cnt', 'price', 'diff_days', 'men_share', 'women_share', 'men_bucket_share_0',\n",
    "       'women_bucket_share_0', 'people_in_bucket_0', 'men_bucket_share_1',\n",
    "       'women_bucket_share_1', 'people_in_bucket_1', 'men_bucket_share_2',\n",
    "       'women_bucket_share_2', 'people_in_bucket_2', 'men_bucket_share_3',\n",
    "       'women_bucket_share_3', 'people_in_bucket_3', 'men_bucket_share_4',\n",
    "       'women_bucket_share_4', 'people_in_bucket_4', 'men_bucket_share_5',\n",
    "       'women_bucket_share_5', 'people_in_bucket_5', 'men_bucket_share_6',\n",
    "       'women_bucket_share_6', 'people_in_bucket_6']\n",
    "cat_features = ['region_name', 'city_name', 'cpe_manufacturer_name', 'cpe_model_name',\n",
    "                'cpe_type_cd', 'cpe_model_os_type', 'part_of_day', \n",
    "                'url_host','dow'] #\n",
    "cat_embs_size = [(81, 3), (985, 3), (37, 6), (599, 32),\n",
    "                 (4, 2), (3, 2), (4, 2), \n",
    "                 (23761, 200),  (7, 3)] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f278545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "\n",
    "class AlphaDataset(Dataset):\n",
    "    def __init__(self, df, is_train = False):\n",
    "        self.df = df.copy()\n",
    "        self.is_train = is_train\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self,idx):\n",
    "        s = self.df.iloc[idx]\n",
    "        uid = s.user_id\n",
    "        \n",
    "        cat_features_vals = np.stack(s[cat_features].values).T\n",
    "        num_features_vals = np.stack(s[num_features].values).T\n",
    "        idxs = np.arange(len(s['url_host']))\n",
    "        np.random.shuffle(idxs)\n",
    "        cat_features_vals = cat_features_vals[idxs]\n",
    "        num_features_vals = num_features_vals[idxs]\n",
    "        urls = np.array(s['url_host'])[idxs]\n",
    "        n = len(s['url_host'])\n",
    "        hist_len = math.ceil(0.8 * n)\n",
    "        target_len = math.ceil(0.8 * n)\n",
    "        cat_features_vals = cat_features_vals[:hist_len]\n",
    "        num_features_vals = num_features_vals[:hist_len]\n",
    "        mask = np.ones(hist_len)\n",
    "        target = np.zeros(23761)\n",
    "        target[urls[-target_len:]] = 1\n",
    "        \n",
    "        return cat_features_vals, num_features_vals, target\n",
    "    \n",
    "\n",
    "def pad_matrix(mat, max_len):\n",
    "    n = mat.shape[0]\n",
    "    if len(mat.shape) == 1:\n",
    "        if max_len <= n:\n",
    "            return torch.tensor(mat[-max_len:])\n",
    "        return torch.cat([torch.tensor(mat), torch.zeros(max_len - n)])\n",
    "    if max_len <= n:\n",
    "        return torch.tensor(mat[-max_len:])\n",
    "    return torch.cat([\n",
    "        torch.tensor(mat),\n",
    "        torch.zeros((max_len - n, mat.shape[1]))\n",
    "    ])\n",
    "\n",
    "def collate_batch(batch):\n",
    "    #cat_features_vals, num_features_vals, user_features_vals, mask, targets = batch\n",
    "    max_len = 0\n",
    "    for c in batch:\n",
    "        if c[0].shape[0] > max_len:\n",
    "            max_len = c[0].shape[0]\n",
    "            \n",
    "    if max_len > 1200:\n",
    "        max_len = 1200\n",
    "\n",
    "    return torch.stack([\n",
    "        pad_matrix(m[0], max_len) for m in batch\n",
    "    ]), torch.stack([\n",
    "        pad_matrix(m[1], max_len) for m in batch\n",
    "    ]), torch.stack([torch.tensor(m[2]) for m in batch])\n",
    "      \n",
    "#dataloader = DataLoader(ds_test, batch_size=64, collate_fn=collate_batch, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2985c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q['data_len'] = q['region_name'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1c9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class BySequenceLengthSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size=64):\n",
    "        ind_n_len = []\n",
    "        pps = data_source['data_len'].values\n",
    "        for i, p in enumerate(pps):\n",
    "            ind_n_len.append( (i, p) )\n",
    "        self.ind_n_len = ind_n_len\n",
    "        self.bucket_boundaries = np.quantile(pps, [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8])\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        data_buckets = dict()\n",
    "        # where p is the id number and seq_len is the length of this id number. \n",
    "        for p, seq_len in self.ind_n_len:\n",
    "            pid = self.element_to_bucket_id(p,seq_len)\n",
    "            if pid in data_buckets.keys():\n",
    "                data_buckets[pid].append(p)\n",
    "            else:\n",
    "                data_buckets[pid] = [p]\n",
    "\n",
    "        for k in data_buckets.keys():\n",
    "            data_buckets[k] = np.asarray(data_buckets[k])\n",
    "\n",
    "        iter_list = []\n",
    "        for k in data_buckets.keys():\n",
    "            np.random.shuffle(data_buckets[k])\n",
    "            iter_list += (np.array_split(data_buckets[k]\n",
    "                           , int(data_buckets[k].shape[0]/self.batch_size)))\n",
    "        shuffle(iter_list) # shuffle all the batches so they arent ordered by bucket\n",
    "        # size\n",
    "        for i in iter_list: \n",
    "            yield i.tolist() # as it was stored in an array\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_source.shape[0]\n",
    "    \n",
    "    def element_to_bucket_id(self, x, seq_length):\n",
    "        boundaries = list(self.bucket_boundaries)\n",
    "        buckets_min = [np.iinfo(np.int32).min] + boundaries\n",
    "        buckets_max = boundaries + [np.iinfo(np.int32).max]\n",
    "        conditions_c = np.logical_and(\n",
    "          np.less_equal(buckets_min, seq_length),\n",
    "          np.less(seq_length, buckets_max))\n",
    "        bucket_id = np.min(np.where(conditions_c))\n",
    "        return bucket_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6104ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CFG:\n",
    "    learning_rate=1.0e-3\n",
    "    batch_size=64\n",
    "    num_workers=4\n",
    "    print_freq=100\n",
    "    test_freq=1\n",
    "    start_epoch=0\n",
    "    num_train_epochs=3\n",
    "    warmup_steps=30\n",
    "    max_grad_norm=1000\n",
    "    gradient_accumulation_steps=1\n",
    "    weight_decay=0.01    \n",
    "    dropout=0.0\n",
    "    emb_size=100\n",
    "    hidden_size=160\n",
    "    nlayers=2\n",
    "    nheads=8\n",
    "    seq_len=1200\n",
    "    target_size = 7\n",
    "    num_fts_len = 64\n",
    "    fts_len = sum([x[1] for x in cat_embs_size]) + len(num_features)\n",
    "    \n",
    "class GRUBaseModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GRUBaseModel, self).__init__()\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.embeds = torch.nn.ModuleList([\n",
    "            nn.Embedding(a,b) for a,b in cat_embs_size\n",
    "        ])\n",
    "        \n",
    "        input_len = cfg.fts_len\n",
    "\n",
    "        self.encoder = nn.GRU(input_size = cfg.fts_len,\n",
    "                              hidden_size = cfg.hidden_size,\n",
    "                              num_layers = cfg.nlayers,\n",
    "                              batch_first = True,\n",
    "                              bidirectional = True\n",
    "                             )            \n",
    "        def get_reg():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(4*cfg.hidden_size, cfg.hidden_size), \n",
    "                nn.LayerNorm(cfg.hidden_size),\n",
    "                nn.Mish()           \n",
    "            ) \n",
    "        self.reg_layer = get_reg()\n",
    "        self.male_age = nn.Linear(cfg.hidden_size, 7)\n",
    "        self.unsuper  = nn.Linear(cfg.hidden_size, 23761)\n",
    "        \n",
    "    def forward(self, cat_features_vals, num_features_vals):        \n",
    "        batch_size = cat_features_vals.size(0)\n",
    "            \n",
    "        seq_emb = torch.cat([m(cat_features_vals[:,:,i]) \n",
    "                             for i,m in enumerate(self.embeds)] + [num_features_vals], dim=-1)\n",
    "        \n",
    "        _, sequence_output = self.encoder(seq_emb)\n",
    "        sequence_output = sequence_output.transpose(0,1).flatten(start_dim=1)\n",
    "        x = sequence_output\n",
    "        emb = self.reg_layer(x)\n",
    "        \n",
    "        male_age = self.male_age(emb)\n",
    "        unsuper = self.unsuper(emb)\n",
    "        \n",
    "        return male_age, unsuper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a586c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=256\n",
    "ds_train = AlphaDataset(q)\n",
    "sampler = BySequenceLengthSampler(q, bs)\n",
    "train_dl = DataLoader(ds_train, batch_size=1, \n",
    "                      batch_sampler=sampler, \n",
    "                      num_workers=4,\n",
    "                      collate_fn=collate_batch,\n",
    "                      drop_last=False, \n",
    "                      pin_memory=False)\n",
    "\n",
    "model = GRUBaseModel(CFG).cuda()\n",
    "model.load_state_dict(torch.load('./ckps_mms/base_model_20_54ep.pth'))\n",
    "loss_ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "best_loss = 1e9\n",
    "for epoch in range(60):\n",
    "    losses = []\n",
    "    for x in train_dl:\n",
    "        _, out = model(x[0].long().cuda(), x[1].float().cuda())\n",
    "        loss = loss_ce(out, x[2].float().cuda())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(epoch, mean_loss)\n",
    "    if mean_loss < best_loss:\n",
    "        torch.save(model.state_dict(), f'./ckps_mms/base_model_20_60ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9b57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
